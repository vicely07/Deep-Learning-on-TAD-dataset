(10384, 251)
(10384, 250)
(10384,)
(1000, 251)
(1000, 250)
(1000,)
shape of y_train and y_test before categorical
(10384,)
(1000,)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 100)               25100     
_________________________________________________________________
activation_1 (Activation)    (None, 100)               0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 101       
_________________________________________________________________
activation_2 (Activation)    (None, 1)                 0         
=================================================================
Total params: 25,201
Trainable params: 25,201
Non-trainable params: 0
_________________________________________________________________
None
Train on 8307 samples, validate on 2077 samples
Epoch 1/100

 100/8307 [..............................] - ETA: 18s - loss: 1.3461 - acc: 0.3900
2800/8307 [=========>....................] - ETA: 0s - loss: 0.8906 - acc: 0.5371 
5700/8307 [===================>..........] - ETA: 0s - loss: 0.8249 - acc: 0.5575
8307/8307 [==============================] - 0s 50us/step - loss: 0.8014 - acc: 0.5600 - val_loss: 0.9862 - val_acc: 0.2980

Epoch 00001: val_loss improved from inf to 0.98623, saving model to best_model.hdf5
Epoch 2/100

 100/8307 [..............................] - ETA: 0s - loss: 0.7781 - acc: 0.5500
2300/8307 [=======>......................] - ETA: 0s - loss: 0.7219 - acc: 0.5700
4700/8307 [===============>..............] - ETA: 0s - loss: 0.7100 - acc: 0.5900
7000/8307 [========================>.....] - ETA: 0s - loss: 0.7050 - acc: 0.5979
8307/8307 [==============================] - 0s 24us/step - loss: 0.7049 - acc: 0.5967 - val_loss: 0.9457 - val_acc: 0.2821

Epoch 00002: val_loss improved from 0.98623 to 0.94575, saving model to best_model.hdf5
Epoch 3/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6122 - acc: 0.7300
2500/8307 [========>.....................] - ETA: 0s - loss: 0.6737 - acc: 0.6220
4800/8307 [================>.............] - ETA: 0s - loss: 0.6811 - acc: 0.6125
7000/8307 [========================>.....] - ETA: 0s - loss: 0.6745 - acc: 0.6160
8307/8307 [==============================] - 0s 25us/step - loss: 0.6730 - acc: 0.6190 - val_loss: 1.0251 - val_acc: 0.2147

Epoch 00003: val_loss did not improve from 0.94575
Epoch 4/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6866 - acc: 0.6000
2500/8307 [========>.....................] - ETA: 0s - loss: 0.6634 - acc: 0.6312
5000/8307 [=================>............] - ETA: 0s - loss: 0.6584 - acc: 0.6294
7500/8307 [==========================>...] - ETA: 0s - loss: 0.6544 - acc: 0.6335
8307/8307 [==============================] - 0s 23us/step - loss: 0.6544 - acc: 0.6328 - val_loss: 0.9278 - val_acc: 0.2764

Epoch 00004: val_loss improved from 0.94575 to 0.92782, saving model to best_model.hdf5
Epoch 5/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6554 - acc: 0.6400
2500/8307 [========>.....................] - ETA: 0s - loss: 0.6555 - acc: 0.6240
4900/8307 [================>.............] - ETA: 0s - loss: 0.6540 - acc: 0.6273
7200/8307 [=========================>....] - ETA: 0s - loss: 0.6483 - acc: 0.6364
8307/8307 [==============================] - 0s 25us/step - loss: 0.6485 - acc: 0.6361 - val_loss: 0.9658 - val_acc: 0.2417

Epoch 00005: val_loss did not improve from 0.92782
Epoch 6/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6097 - acc: 0.7100
2100/8307 [======>.......................] - ETA: 0s - loss: 0.6479 - acc: 0.6395
4200/8307 [==============>...............] - ETA: 0s - loss: 0.6504 - acc: 0.6360
6200/8307 [=====================>........] - ETA: 0s - loss: 0.6442 - acc: 0.6431
8300/8307 [============================>.] - ETA: 0s - loss: 0.6404 - acc: 0.6454
8307/8307 [==============================] - 0s 27us/step - loss: 0.6402 - acc: 0.6457 - val_loss: 1.0712 - val_acc: 0.1801

Epoch 00006: val_loss did not improve from 0.92782
Epoch 7/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6919 - acc: 0.5900
2400/8307 [=======>......................] - ETA: 0s - loss: 0.6320 - acc: 0.6483
4800/8307 [================>.............] - ETA: 0s - loss: 0.6328 - acc: 0.6458
7700/8307 [==========================>...] - ETA: 0s - loss: 0.6376 - acc: 0.6431
8307/8307 [==============================] - 0s 22us/step - loss: 0.6362 - acc: 0.6437 - val_loss: 0.9570 - val_acc: 0.2480

Epoch 00007: val_loss did not improve from 0.92782
Epoch 8/100

 100/8307 [..............................] - ETA: 0s - loss: 0.5729 - acc: 0.7100
3000/8307 [=========>....................] - ETA: 0s - loss: 0.6327 - acc: 0.6567
5600/8307 [===================>..........] - ETA: 0s - loss: 0.6338 - acc: 0.6493
8300/8307 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.6449
8307/8307 [==============================] - 0s 20us/step - loss: 0.6365 - acc: 0.6449 - val_loss: 0.8265 - val_acc: 0.3587

Epoch 00008: val_loss improved from 0.92782 to 0.82654, saving model to best_model.hdf5
Epoch 9/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6277 - acc: 0.6700
2900/8307 [=========>....................] - ETA: 0s - loss: 0.6338 - acc: 0.6514
5900/8307 [====================>.........] - ETA: 0s - loss: 0.6315 - acc: 0.6527
8307/8307 [==============================] - 0s 19us/step - loss: 0.6340 - acc: 0.6499 - val_loss: 0.9646 - val_acc: 0.2388

Epoch 00009: val_loss did not improve from 0.82654
Epoch 10/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6471 - acc: 0.6500
3200/8307 [==========>...................] - ETA: 0s - loss: 0.6368 - acc: 0.6425
6300/8307 [=====================>........] - ETA: 0s - loss: 0.6262 - acc: 0.6533
8307/8307 [==============================] - 0s 19us/step - loss: 0.6272 - acc: 0.6521 - val_loss: 0.9390 - val_acc: 0.2638

Epoch 00010: val_loss did not improve from 0.82654
Epoch 11/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6374 - acc: 0.6200
3100/8307 [==========>...................] - ETA: 0s - loss: 0.6248 - acc: 0.6632
6000/8307 [====================>.........] - ETA: 0s - loss: 0.6268 - acc: 0.6573
8307/8307 [==============================] - 0s 19us/step - loss: 0.6248 - acc: 0.6594 - val_loss: 1.0085 - val_acc: 0.2080

Epoch 00011: val_loss did not improve from 0.82654
Epoch 12/100

 100/8307 [..............................] - ETA: 0s - loss: 0.5690 - acc: 0.7300
3100/8307 [==========>...................] - ETA: 0s - loss: 0.6172 - acc: 0.6632
6100/8307 [=====================>........] - ETA: 0s - loss: 0.6187 - acc: 0.6643
8307/8307 [==============================] - 0s 19us/step - loss: 0.6208 - acc: 0.6622 - val_loss: 0.9596 - val_acc: 0.2441

Epoch 00012: val_loss did not improve from 0.82654
Epoch 13/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6035 - acc: 0.7100
3200/8307 [==========>...................] - ETA: 0s - loss: 0.6226 - acc: 0.6556
5900/8307 [====================>.........] - ETA: 0s - loss: 0.6171 - acc: 0.6617
8307/8307 [==============================] - 0s 20us/step - loss: 0.6196 - acc: 0.6609 - val_loss: 0.8743 - val_acc: 0.3139

Epoch 00013: val_loss did not improve from 0.82654
Epoch 14/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6644 - acc: 0.6500
2800/8307 [=========>....................] - ETA: 0s - loss: 0.6164 - acc: 0.6646
5700/8307 [===================>..........] - ETA: 0s - loss: 0.6193 - acc: 0.6591
8307/8307 [==============================] - 0s 19us/step - loss: 0.6155 - acc: 0.6633 - val_loss: 0.9523 - val_acc: 0.2595

Epoch 00014: val_loss did not improve from 0.82654
Epoch 15/100

 100/8307 [..............................] - ETA: 0s - loss: 0.6969 - acc: 0.5700
3200/8307 [==========>...................] - ETA: 0s - loss: 0.6091 - acc: 0.6684
6300/8307 [=====================>........] - ETA: 0s - loss: 0.6171 - acc: 0.6638
8307/8307 [==============================] - 0s 18us/step - loss: 0.6159 - acc: 0.6628 - val_loss: 0.8616 - val_acc: 0.3327

Epoch 00015: val_loss did not improve from 0.82654
Epoch 16/100

 100/8307 [..............................] - ETA: 0s - loss: 0.5963 - acc: 0.6800
3100/8307 [==========>...................] - ETA: 0s - loss: 0.6179 - acc: 0.6619
6000/8307 [====================>.........] - ETA: 0s - loss: 0.6101 - acc: 0.6737
8307/8307 [==============================] - 0s 20us/step - loss: 0.6124 - acc: 0.6710 - val_loss: 0.8591 - val_acc: 0.3390

Epoch 00016: val_loss did not improve from 0.82654
predicted class  [[0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [1]
 [1]
 [1]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [1]
 [0]
 [1]
 [1]
 [1]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [1]
 [0]
 [0]
 [1]
 [0]
 [0]
 [0]
 [0]]
Dimensions of pred_class  (1000, 1)
Dimensions of pred_p  (1000, 1)
predicted probabilities of first 5 
 [[0.22207092]
 [0.39643398]
 [0.16618766]
 [0.44880006]
 [0.18126236]]
testing accuracy 0.739
training accuracy 0.7485554699537751
